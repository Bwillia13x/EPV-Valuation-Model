Plan: Integrating AI into EPV Valuation Model
This document outlines potential ways to integrate AI functionalities (using Gemini API or OpenAI API) into the existing Bruce Greenwald EPV Valuation Model, along with technical considerations and next steps.

I. Potential AI Use Cases & Enhancements
Integrating AI can help automate or augment parts of the valuation process that are typically qualitative or require sifting through large amounts of text-based information.
Qualitative Analysis & Sentiment:
News Sentiment Analysis:
Functionality: Fetch recent news articles related to the stock ticker. Use an AI model to analyze the sentiment (positive, negative, neutral) of these articles.
Benefit: Provides a quick gauge of market perception and potential short-term catalysts or headwinds not captured by purely quantitative EPV.
Input to AI: News headlines/summaries.
Output from AI: Sentiment score, summary of key sentiment drivers.
Earnings Call Transcript Analysis:
Functionality: If transcripts are available (e.g., via Financial Modeling Prep or other sources), use AI to summarize key themes, management tone, Q&A insights, and identify forward-looking statements.
Benefit: Deeper insight into management's perspective and operational details.
Input to AI: Earnings call transcript text.
Output from AI: Summaries, key topics, sentiment.
Moat & Risk Identification (from Text):
Functionality: Analyze company descriptions, annual reports (10-K sections like "Risk Factors" or "Business"), or news to identify potential competitive advantages (moat) or significant risks.
Benefit: Augments the quantitative EPV by highlighting qualitative factors crucial for long-term value.
Input to AI: Relevant text sections.
Output from AI: List of potential moats/risks with brief explanations.
Data Extraction & Interpretation (Advanced):
Identifying Non-Recurring Items:
Functionality: Analyze footnotes or MD&A sections of financial reports to help identify and quantify non-recurring items (e.g., restructuring charges, legal settlements) for better earnings normalization.
Benefit: Improves the accuracy of "Normalized EBIT," a critical input for EPV. This is a very challenging task for AI and would require sophisticated prompting and possibly fine-tuning.
Input to AI: Financial statement footnotes, MD&A text.
Output from AI: Potential non-recurring items and their estimated impact.
Summarizing MD&A:
Functionality: Provide a concise summary of the Management Discussion & Analysis section, highlighting key performance drivers, challenges, and outlook.
Benefit: Saves time in understanding management's narrative.
Input to AI: MD&A text.
Output from AI: Bullet-point summary.
Assumption Justification & Context:
Market & Economic Context for Assumptions:
Functionality: Use AI to fetch and summarize current market commentary or economic indicators relevant to assumptions like the Equity Risk Premium (ERP) or industry-specific growth outlook (even if EPV is no-growth, understanding the environment is key).
Benefit: Helps in making more informed choices for subjective inputs.
Input to AI: Queries like "current consensus on US Equity Risk Premium" or "economic outlook for the semiconductor industry."
Output from AI: Summarized information, links to sources.
Enhanced Reporting & Narrative Generation:
Automated Report Sections:
Functionality: Generate narrative sections for the final valuation report, combining the quantitative EPV results with qualitative insights derived from AI (e.g., news sentiment, risk summary).
Benefit: Creates a more comprehensive and readable valuation output.
Input to AI: EPV results, AI-derived qualitative summaries, ticker.
Output from AI: Draft text for report sections (e.g., "Qualitative Overview," "Risk Assessment").
Interactive Q&A / Chatbot (Ambitious):
Functionality: Allow users to ask questions about the valuation results or the company in natural language, with AI providing answers based on the model's data and fetched information.
Benefit: Makes the model more interactive and user-friendly.
Input to AI: User questions, context from the model.
Output from AI: Natural language answers.
II. Technical Considerations
API Choice & Setup:
Gemini API vs. OpenAI API: Both are powerful. Choice might depend on familiarity, specific model capabilities preferred, pricing, and ease of integration.
API Keys: Securely store and manage API keys (e.g., using .env file and python-dotenv, then loading into config.py or directly in the new AI module).
Client Libraries: Install the respective Python client libraries (google-generativeai for Gemini, openai for OpenAI). Add to requirements.txt.
New Module(s):
Consider creating a new module, e.g., ai_analyzer.py or qualitative_analyzer.py, to encapsulate all AI-related functions.
This module would handle API calls, prompt construction, and parsing of AI responses.
Prompt Engineering:
This will be critical. The quality of AI output heavily depends on how well the prompts are crafted.
Prompts need to be clear, specific, and provide sufficient context.
Iterative refinement of prompts will be necessary.
Data Input for AI:
Decide what data to pass to the AI models (e.g., just the ticker, pre-fetched news summaries, specific financial figures, sections of reports).
Consider token limits for the AI models. Large texts (like full annual reports) might need to be chunked or summarized before sending.
Parsing AI Output:
AI models typically return JSON or text. Functions will be needed to parse this output into a usable format (e.g., extracting sentiment scores, lists of risks, summary text).
Cost Management:
API calls to LLMs are not free. Be mindful of the number of calls, token usage, and associated costs, especially during development and testing.
Implement caching for AI responses where appropriate if the same information is requested multiple times for the same inputs.
Error Handling & Rate Limiting:
Implement robust error handling for API calls (network issues, API errors, rate limits).
Be aware of API rate limits and implement retries with backoff if necessary.
Fetching External Content (News, Reports):
For use cases like news sentiment or report analysis, you'll first need a way to fetch this content. This might involve:
Using existing APIs (e.g., news APIs, Financial Modeling Prep for transcripts/SEC filings).
Web scraping (can be fragile and legally complex).
III. Proposed Integration Steps & Plan Update
This plan integrates with your existing "EPV Model - Testing and Next Steps Plan" (id: cursor_testing_plan_v1).
Complete Phase 1-3 (Testing & Debugging) of cursor_testing_plan_v1: Ensure the current quantitative model is stable and tests are passing. This is a prerequisite.
Create tests/test_utils.py: (As per Phase 4, Prompt 2 of cursor_testing_plan_v1).
Select an Initial AI Feature: Choose one or two relatively simple AI features to start with. Good candidates might be:
News Sentiment Analysis (if a news source is easily accessible).
Generating a Qualitative Company Summary based on its yfinance info['longBusinessSummary'].
Develop the AI Integration for the Selected Feature(s):
Action for Cursor Agent: "Create a new file epv_valuation_model/ai_analyzer.py."
Action for Cursor Agent: "In ai_analyzer.py, implement a function to [Chosen Feature, e.g., 'get_company_summary_from_ai'] using the [Gemini/OpenAI] API. This will involve:
Setting up the API client (including API key handling, ideally loaded from config.py which in turn loads from .env).
Crafting a prompt.
Making the API call.
Parsing the response."
Action for Cursor Agent: "Add the chosen API client library (e.g., google-generativeai or openai) to requirements.txt and ensure it's installed in the virtual environment."
Action for Cursor Agent: "Update config.py to store the AI API key (loaded from .env)."
Integrate AI Output into main.py and reporting.py:
Action for Cursor Agent: "Modify main.py to call the new function in ai_analyzer.py after the core EPV calculations."
Action for Cursor Agent: "Modify reporting.py (specifically generate_text_summary) to include the AI-generated insights in the output report."
Testing the AI Integration:
Manually test the new feature with a few tickers.
Action for Cursor Agent: "Create a new test file tests/test_ai_analyzer.py. Write initial tests for the AI functions. These tests will likely need to use mocking for the AI API calls to be fast, deterministic, and avoid API costs during automated testing."
Context: Testing AI outputs for exactness is hard. Tests often focus on:
Does the function run without error?
Does it return the expected data structure?
For mocked calls, does it construct the prompt correctly?
Does it parse a sample (mocked) AI response correctly?
Iterate and Expand: Based on the success of the initial feature, plan and implement further AI integrations from the list above.
This AI integration is a significant enhancement. Starting with a focused feature, getting the API plumbing right, and then expanding is a good approach. Remember to prioritize robust testing, especially with the introduction of external API calls.

